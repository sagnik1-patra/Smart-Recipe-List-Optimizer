{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e07e92-ced6-4f64-b897-47bd417505c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Scores file not found:\nC:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\\scores_full.csv\n\nCreate it first (columns: id,name,score).",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Scores file not found:\nC:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\\scores_full.csv\n\nCreate it first (columns: id,name,score).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# One-cell Accuracy/F1 vs Threshold + Confusion Matrix (robust to messy labels)\n",
    "# Requires: a scores CSV with columns: id,name,score\n",
    "# Requires: a labels CSV with columns: id,label  (label = 1/0 or yes/no/selected/rejected etc.)\n",
    "# If labels.csv is missing/empty, this cell will write a template you can fill, then re-run.\n",
    "\n",
    "# If needed once:\n",
    "# !pip install pandas numpy scikit-learn matplotlib\n",
    "\n",
    "import os, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# CONFIG — edit if needed\n",
    "# =========================\n",
    "BASE_DIR      = r\"C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\"\n",
    "SCORES_CSV    = str(Path(BASE_DIR) / \"scores_full.csv\")   # put your scores file here (id,name,score)\n",
    "LABELS_CSV    = str(Path(BASE_DIR) / \"labels.csv\")        # labels file (id,label). Auto-created if missing.\n",
    "ALIGNED_PREV  = str(Path(BASE_DIR) / \"labels_aligned_preview.csv\")\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8\", \"utf-16\", \"latin-1\", \"cp1252\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, encoding_errors=\"ignore\", engine=\"python\")\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, engine=\"python\")\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def normalize_bool(val):\n",
    "    if pd.isna(val): return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    true_vals  = {\"1\",\"true\",\"yes\",\"y\",\"selected\",\"hire\",\"hired\",\"positive\",\"pos\",\"shortlisted\",\"good\",\"relevant\"}\n",
    "    false_vals = {\"0\",\"false\",\"no\",\"n\",\"rejected\",\"reject\",\"negative\",\"neg\",\"not selected\",\"bad\",\"irrelevant\",\"non-relevant\"}\n",
    "    if s in true_vals:  return 1\n",
    "    if s in false_vals: return 0\n",
    "    try:\n",
    "        f = float(s);  return 1 if f >= 0.5 else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def unify_name(s: str) -> str:\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def write_labels_template_from_scores(scores_df: pd.DataFrame, out_path: str):\n",
    "    tmpl = scores_df[[\"id\",\"name\",\"score\"]].copy()\n",
    "    tmpl[\"label\"] = \"\"  # to be filled 1/0\n",
    "    tmpl = tmpl.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    tmpl.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# =========================\n",
    "# 1) Load scores\n",
    "# =========================\n",
    "if not Path(SCORES_CSV).exists():\n",
    "    raise SystemExit(f\"Scores file not found:\\n{SCORES_CSV}\\n\\nCreate it first (columns: id,name,score).\")\n",
    "\n",
    "scores = read_csv_robust(SCORES_CSV)\n",
    "scores.columns = [c.strip().lower() for c in scores.columns]\n",
    "need_cols = {\"id\",\"name\",\"score\"}\n",
    "if not need_cols.issubset(set(scores.columns)):\n",
    "    raise SystemExit(f\"{SCORES_CSV} must contain columns {need_cols}. Found: {list(scores.columns)}\")\n",
    "scores = scores[[\"id\",\"name\",\"score\"]].copy()\n",
    "\n",
    "# =========================\n",
    "# 2) Load / fix labels\n",
    "# =========================\n",
    "labels_path = Path(LABELS_CSV)\n",
    "if not labels_path.exists():\n",
    "    write_labels_template_from_scores(scores, LABELS_CSV)\n",
    "    print(f\"[TEMPLATE WRITTEN] → {LABELS_CSV}\")\n",
    "    print(\"Open it in Excel, fill the 'label' column with 1 (positive) / 0 (negative), save, and re-run this cell.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "labels = read_csv_robust(LABELS_CSV)\n",
    "labels.columns = [c.strip().lower() for c in labels.columns]\n",
    "\n",
    "# standardize label column name if user used a synonym\n",
    "if \"label\" not in labels.columns:\n",
    "    for alt in [\"labels\",\"target\",\"y\",\"class\"]:\n",
    "        if alt in labels.columns:\n",
    "            labels = labels.rename(columns={alt:\"label\"})\n",
    "            break\n",
    "if \"label\" not in labels.columns:\n",
    "    # rewrite a proper template and exit\n",
    "    write_labels_template_from_scores(scores, LABELS_CSV)\n",
    "    print(f\"[TEMPLATE REWRITTEN] → {LABELS_CSV}\")\n",
    "    print(\"Reason: labels.csv had no 'label' column. Fill it with 1/0 and re-run.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "# ensure we have an id column; if missing, try to recover by name\n",
    "if \"id\" not in labels.columns:\n",
    "    if \"name\" in labels.columns:\n",
    "        labels[\"name_key\"] = labels[\"name\"].astype(str).map(unify_name)\n",
    "        sc = scores.copy(); sc[\"name_key\"] = sc[\"name\"].astype(str).map(unify_name)\n",
    "        recovered = pd.merge(labels, sc[[\"name_key\",\"id\"]], how=\"left\", on=\"name_key\")\n",
    "        if recovered[\"id\"].notna().any():\n",
    "            labels = recovered.drop(columns=[\"name_key\"])\n",
    "            print(\"[FIX] Recovered 'id' in labels.csv by matching normalized names.\")\n",
    "        else:\n",
    "            write_labels_template_from_scores(scores, LABELS_CSV)\n",
    "            print(f\"[TEMPLATE REWRITTEN] → {LABELS_CSV}\")\n",
    "            print(\"Reason: labels.csv had no 'id' and couldn't recover via name match. Fill and re-run.\")\n",
    "            raise SystemExit()\n",
    "    else:\n",
    "        write_labels_template_from_scores(scores, LABELS_CSV)\n",
    "        print(f\"[TEMPLATE REWRITTEN] → {LABELS_CSV}\")\n",
    "        print(\"Reason: labels.csv needs at least 'id' OR 'name'. Fill and re-run.\")\n",
    "        raise SystemExit()\n",
    "\n",
    "# check labels present\n",
    "has_any_label = labels[\"label\"].notna().any() and (labels[\"label\"].astype(str).str.strip() != \"\").any()\n",
    "if not has_any_label:\n",
    "    cleaned = labels[[\"id\"]].merge(scores[[\"id\",\"name\",\"score\"]], how=\"left\", on=\"id\")\n",
    "    cleaned[\"label\"] = \"\"\n",
    "    cleaned.to_csv(LABELS_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[TEMPLATE UPDATED] → {LABELS_CSV}\")\n",
    "    print(\"Please fill 'label' (1/0) for the rows you want to evaluate, save, then re-run this cell.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "# =========================\n",
    "# 3) Align by id and normalize labels\n",
    "# =========================\n",
    "labels[\"label_bin\"] = labels[\"label\"].map(normalize_bool)\n",
    "labels = labels.dropna(subset=[\"label_bin\"]).copy()\n",
    "labels[\"label_bin\"] = labels[\"label_bin\"].astype(int)\n",
    "\n",
    "merged = pd.merge(scores, labels[[\"id\",\"label_bin\"]], how=\"inner\", on=\"id\")\n",
    "if merged.empty:\n",
    "    # Show debug to help fix\n",
    "    print(\"[DEBUG] Example IDs from scores:\", scores[\"id\"].head(5).tolist())\n",
    "    print(\"[DEBUG] Example IDs from labels:\", labels[\"id\"].head(5).tolist())\n",
    "    raise SystemExit(\"No overlap between labels and scores by 'id'. Ensure you labeled rows from the template generated here.\")\n",
    "\n",
    "merged.to_csv(ALIGNED_PREV, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Aligned {len(merged)} labeled rows. Preview → {ALIGNED_PREV}\")\n",
    "\n",
    "# =========================\n",
    "# 4) Plot Accuracy/F1 vs Threshold and Confusion Matrix\n",
    "# =========================\n",
    "y_true  = merged[\"label_bin\"].astype(int).values\n",
    "y_score = merged[\"score\"].astype(float).values\n",
    "\n",
    "thresholds = np.linspace(0.0, 1.0, 201)\n",
    "accs, f1s = [], []\n",
    "for t in thresholds:\n",
    "    y_pred = (y_score >= t).astype(int)\n",
    "    accs.append(accuracy_score(y_true, y_pred))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    f1s.append(f1)\n",
    "\n",
    "best_idx = int(np.argmax(accs))\n",
    "best_t   = float(thresholds[best_idx])\n",
    "best_acc = float(accs[best_idx])\n",
    "best_f1  = float(f1s[best_idx])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, accs, label=\"Accuracy\")\n",
    "plt.plot(thresholds, f1s, label=\"F1\")\n",
    "plt.axvline(best_t, linestyle=\"--\", label=f\"Best T={best_t:.3f}\")\n",
    "plt.title(\"Accuracy / F1 vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best threshold: {best_t:.3f} | Accuracy={best_acc:.4f} | F1={best_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix @ best threshold\n",
    "y_pred_best = (y_score >= best_t).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred_best, labels=[0,1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, aspect=\"auto\")\n",
    "plt.title(\"Confusion Matrix (Heatmap)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xticks([0,1], [\"0 (Neg)\",\"1 (Pos)\"])\n",
    "plt.yticks([0,1], [\"0 (Neg)\",\"1 (Pos)\"])\n",
    "for (i,j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha='center', va='center', fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"TN={tn}  FP={fp}  FN={fn}  TP={tp}\")\n",
    "\n",
    "# Optional: ROC & PR curves\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Skipped ROC/PR:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d429d8-345d-47e0-86f1-ee558171181f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
