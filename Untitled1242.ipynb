{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4fe639-9b1a-4ab4-8cf7-287f6a644de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.indexes.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m df_val     = pd.read_csv(INTER_VAL)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(INGR_MAP, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     ingr_map = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecipes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_recipes.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Interactions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_inter.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Ingr_map: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ingr_map)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Clean and preprocess recipes\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas.core.indexes.numeric'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Smart Grocery List Optimizer - Data Preprocessing Pipeline\n",
    "# Saves embeddings & configs into multiple formats\n",
    "\n",
    "import os, json, yaml, pickle, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import h5py\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\"\n",
    "ARCHIVE  = os.path.join(BASE_DIR, \"archive\")\n",
    "\n",
    "OUT_H5   = os.path.join(BASE_DIR, \"recipes_embeddings.h5\")\n",
    "OUT_PKL  = os.path.join(BASE_DIR, \"recipes_embeddings.pkl\")\n",
    "OUT_JSON = os.path.join(BASE_DIR, \"recipes_embeddings.json\")\n",
    "OUT_YAML = os.path.join(BASE_DIR, \"recipes_embeddings.yaml\")\n",
    "\n",
    "# Dataset paths\n",
    "INGR_MAP   = os.path.join(ARCHIVE, \"ingr_map.pkl\")\n",
    "RAW_RECIP  = os.path.join(ARCHIVE, \"RAW_recipes.csv\")\n",
    "RAW_INTER  = os.path.join(ARCHIVE, \"RAW_interactions.csv\")\n",
    "PP_RECIP   = os.path.join(ARCHIVE, \"PP_recipes.csv\")\n",
    "PP_USERS   = os.path.join(ARCHIVE, \"PP_users.csv\")\n",
    "INTER_TRAIN = os.path.join(ARCHIVE, \"interactions_train.csv\")\n",
    "INTER_TEST  = os.path.join(ARCHIVE, \"interactions_test.csv\")\n",
    "INTER_VAL   = os.path.join(ARCHIVE, \"interactions_validation.csv\")\n",
    "\n",
    "# =========================\n",
    "# Load datasets\n",
    "# =========================\n",
    "print(\"[INFO] Loading datasets...\")\n",
    "df_recipes = pd.read_csv(RAW_RECIP)\n",
    "df_inter   = pd.read_csv(RAW_INTER)\n",
    "df_pp_rec  = pd.read_csv(PP_RECIP)\n",
    "df_pp_users= pd.read_csv(PP_USERS)\n",
    "\n",
    "df_train   = pd.read_csv(INTER_TRAIN)\n",
    "df_test    = pd.read_csv(INTER_TEST)\n",
    "df_val     = pd.read_csv(INTER_VAL)\n",
    "\n",
    "with open(INGR_MAP, \"rb\") as f:\n",
    "    ingr_map = pickle.load(f)\n",
    "\n",
    "print(f\"Recipes: {df_recipes.shape}, Interactions: {df_inter.shape}, Ingr_map: {len(ingr_map)}\")\n",
    "\n",
    "# =========================\n",
    "# Clean and preprocess recipes\n",
    "# =========================\n",
    "def clean_ingredients(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.lower().replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\" \")\n",
    "    return \"\"\n",
    "\n",
    "df_recipes[\"clean_ingredients\"] = df_recipes[\"ingredients\"].astype(str).map(clean_ingredients)\n",
    "\n",
    "# =========================\n",
    "# Vectorize recipes (TF-IDF on ingredients)\n",
    "# =========================\n",
    "print(\"[INFO] Computing TF-IDF embeddings...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_recipes[\"clean_ingredients\"].tolist())\n",
    "X = normalize(X)\n",
    "\n",
    "print(f\"[OK] Embedding matrix shape: {X.shape}\")\n",
    "\n",
    "# =========================\n",
    "# Save artifacts\n",
    "# =========================\n",
    "\n",
    "# Save H5\n",
    "with h5py.File(OUT_H5, \"w\") as hf:\n",
    "    hf.create_dataset(\"embeddings\", data=X.toarray())\n",
    "    hf.create_dataset(\"recipe_ids\", data=df_recipes[\"id\"].astype(str).values.astype(\"S\"))\n",
    "print(f\"[OK] Saved H5 → {OUT_H5}\")\n",
    "\n",
    "# Save PKL\n",
    "with open(OUT_PKL, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": X,\n",
    "        \"recipe_ids\": df_recipes[\"id\"].tolist(),\n",
    "        \"vectorizer\": vectorizer\n",
    "    }, f)\n",
    "print(f\"[OK] Saved PKL → {OUT_PKL}\")\n",
    "\n",
    "# Save JSON\n",
    "json_data = {\n",
    "    \"recipe_ids\": df_recipes[\"id\"].tolist()[:200],  # store sample to keep size small\n",
    "    \"ingredients\": df_recipes[\"clean_ingredients\"].tolist()[:200],\n",
    "    \"shape\": X.shape\n",
    "}\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "print(f\"[OK] Saved JSON → {OUT_JSON}\")\n",
    "\n",
    "# Save YAML\n",
    "yaml_data = {\n",
    "    \"n_recipes\": int(X.shape[0]),\n",
    "    \"embedding_dim\": int(X.shape[1]),\n",
    "    \"vectorizer_params\": vectorizer.get_params(),\n",
    "    \"columns\": list(df_recipes.columns)\n",
    "}\n",
    "with open(OUT_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(yaml_data, f)\n",
    "print(f\"[OK] Saved YAML → {OUT_YAML}\")\n",
    "\n",
    "print(\"[DONE] All artifacts saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c13e56-b1d4-42a9-8415-e51dab42bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.5.3\n",
      "  Downloading pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas==1.5.3) (1.26.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.17.0)\n",
      "Downloading pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/10.3 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.7/10.3 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.4/10.3 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 11.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.1\n",
      "    Uninstalling pandas-2.3.1:\n",
      "      Successfully uninstalled pandas-2.3.1\n",
      "Successfully installed pandas-1.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "geopandas 1.1.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "stable-baselines3 2.6.0 requires torch<3.0,>=2.3, but you have torch 2.2.2 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
      "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas==1.5.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dff366f-d300-4c90-81f2-d8edfa6c4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets...\n",
      "Recipes: (231637, 12), Interactions: (1132367, 5), Ingr_map: 11659\n",
      "[INFO] Computing TF-IDF embeddings...\n",
      "[OK] Embedding matrix shape: (231637, 4171)\n",
      "[OK] Saved H5 → C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\\recipes_embeddings.h5\n",
      "[OK] Saved PKL → C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\\recipes_embeddings.pkl\n",
      "[OK] Saved JSON → C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\\recipes_embeddings.json\n",
      "[OK] Saved YAML → C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\\recipes_embeddings.yaml\n",
      "[DONE] All artifacts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Smart Grocery List Optimizer - Data Preprocessing Pipeline\n",
    "# Saves embeddings & configs into multiple formats\n",
    "\n",
    "import os, json, yaml, pickle, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import h5py\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Smart Grocery List Optimizer\"\n",
    "ARCHIVE  = os.path.join(BASE_DIR, \"archive\")\n",
    "\n",
    "OUT_H5   = os.path.join(BASE_DIR, \"recipes_embeddings.h5\")\n",
    "OUT_PKL  = os.path.join(BASE_DIR, \"recipes_embeddings.pkl\")\n",
    "OUT_JSON = os.path.join(BASE_DIR, \"recipes_embeddings.json\")\n",
    "OUT_YAML = os.path.join(BASE_DIR, \"recipes_embeddings.yaml\")\n",
    "\n",
    "# Dataset paths\n",
    "INGR_MAP   = os.path.join(ARCHIVE, \"ingr_map.pkl\")\n",
    "RAW_RECIP  = os.path.join(ARCHIVE, \"RAW_recipes.csv\")\n",
    "RAW_INTER  = os.path.join(ARCHIVE, \"RAW_interactions.csv\")\n",
    "PP_RECIP   = os.path.join(ARCHIVE, \"PP_recipes.csv\")\n",
    "PP_USERS   = os.path.join(ARCHIVE, \"PP_users.csv\")\n",
    "INTER_TRAIN = os.path.join(ARCHIVE, \"interactions_train.csv\")\n",
    "INTER_TEST  = os.path.join(ARCHIVE, \"interactions_test.csv\")\n",
    "INTER_VAL   = os.path.join(ARCHIVE, \"interactions_validation.csv\")\n",
    "\n",
    "# =========================\n",
    "# Load datasets\n",
    "# =========================\n",
    "print(\"[INFO] Loading datasets...\")\n",
    "df_recipes = pd.read_csv(RAW_RECIP)\n",
    "df_inter   = pd.read_csv(RAW_INTER)\n",
    "df_pp_rec  = pd.read_csv(PP_RECIP)\n",
    "df_pp_users= pd.read_csv(PP_USERS)\n",
    "\n",
    "df_train   = pd.read_csv(INTER_TRAIN)\n",
    "df_test    = pd.read_csv(INTER_TEST)\n",
    "df_val     = pd.read_csv(INTER_VAL)\n",
    "\n",
    "with open(INGR_MAP, \"rb\") as f:\n",
    "    ingr_map = pickle.load(f)\n",
    "\n",
    "print(f\"Recipes: {df_recipes.shape}, Interactions: {df_inter.shape}, Ingr_map: {len(ingr_map)}\")\n",
    "\n",
    "# =========================\n",
    "# Clean and preprocess recipes\n",
    "# =========================\n",
    "def clean_ingredients(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.lower().replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\",\",\" \")\n",
    "    return \"\"\n",
    "\n",
    "df_recipes[\"clean_ingredients\"] = df_recipes[\"ingredients\"].astype(str).map(clean_ingredients)\n",
    "\n",
    "# =========================\n",
    "# Vectorize recipes (TF-IDF on ingredients)\n",
    "# =========================\n",
    "print(\"[INFO] Computing TF-IDF embeddings...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_recipes[\"clean_ingredients\"].tolist())\n",
    "X = normalize(X)\n",
    "\n",
    "print(f\"[OK] Embedding matrix shape: {X.shape}\")\n",
    "\n",
    "# =========================\n",
    "# Save artifacts\n",
    "# =========================\n",
    "\n",
    "# Save H5\n",
    "with h5py.File(OUT_H5, \"w\") as hf:\n",
    "    hf.create_dataset(\"embeddings\", data=X.toarray())\n",
    "    hf.create_dataset(\"recipe_ids\", data=df_recipes[\"id\"].astype(str).values.astype(\"S\"))\n",
    "print(f\"[OK] Saved H5 → {OUT_H5}\")\n",
    "\n",
    "# Save PKL\n",
    "with open(OUT_PKL, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"embeddings\": X,\n",
    "        \"recipe_ids\": df_recipes[\"id\"].tolist(),\n",
    "        \"vectorizer\": vectorizer\n",
    "    }, f)\n",
    "print(f\"[OK] Saved PKL → {OUT_PKL}\")\n",
    "\n",
    "# Save JSON\n",
    "json_data = {\n",
    "    \"recipe_ids\": df_recipes[\"id\"].tolist()[:200],  # store sample to keep size small\n",
    "    \"ingredients\": df_recipes[\"clean_ingredients\"].tolist()[:200],\n",
    "    \"shape\": X.shape\n",
    "}\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "print(f\"[OK] Saved JSON → {OUT_JSON}\")\n",
    "\n",
    "# Save YAML\n",
    "yaml_data = {\n",
    "    \"n_recipes\": int(X.shape[0]),\n",
    "    \"embedding_dim\": int(X.shape[1]),\n",
    "    \"vectorizer_params\": vectorizer.get_params(),\n",
    "    \"columns\": list(df_recipes.columns)\n",
    "}\n",
    "with open(OUT_YAML, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(yaml_data, f)\n",
    "print(f\"[OK] Saved YAML → {OUT_YAML}\")\n",
    "\n",
    "print(\"[DONE] All artifacts saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cfb1d-0e98-4fb0-9017-c34ca9e9bef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
